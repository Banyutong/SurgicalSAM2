{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Points Prompt for COCO format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as maskUtils\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "if torch.cuda.get_device_properties(0).major >= 8:\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../SurgicalSAM2/checkpoints/sam2_hiera_tiny.pt\"\n",
    "model_cfg = \"sam2_hiera_t.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels == 1]\n",
    "    neg_points = coords[labels == 0]\n",
    "    ax.scatter(\n",
    "        pos_points[:, 0],\n",
    "        pos_points[:, 1],\n",
    "        color=\"green\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        neg_points[:, 0],\n",
    "        neg_points[:, 1],\n",
    "        color=\"red\",\n",
    "        marker=\"*\",\n",
    "        s=marker_size,\n",
    "        edgecolor=\"white\",\n",
    "        linewidth=1.25,\n",
    "    )\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vedio Dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Video and Frames Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/bd_byta6000i0/users/sam2/kyyang/endoscapes_video.json\",\"r\") as f:\n",
    "    video_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_order = 7\n",
    "video_dir = tempfile.mkdtemp()\n",
    "\n",
    "for idx, frame in enumerate(video_info[video_order]['frames']):\n",
    "    frame_name = formatted_number = str(idx).zfill(8)  # 填充到5位宽度\n",
    "    dst_path = os.path.join(video_dir, f'{frame_name}.jpg')\n",
    "    src_path = frame['path']\n",
    "    os.symlink(src_path,dst_path)\n",
    "# ic(sorted(os.listdir(video_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load frames into predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_file = \"/bd_byta6000i0/users/dataset/MedicalImage/Endoscapes2023/raw/train_seg/annotation_coco.json\"\n",
    "coco = COCO(annotation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(coco.cats)\n",
    "ic(num_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the first annotated image of current video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = coco.getImgIds()\n",
    "imgs = coco.loadImgs(img_ids)\n",
    "for img in imgs:\n",
    "    if img[\"video_id\"] == video_info[video_order][\"video_id\"]:\n",
    "        ann_ids = coco.getAnnIds(imgIds=img[\"id\"])\n",
    "        if ann_ids == []:\n",
    "            continue\n",
    "        first_frame = img\n",
    "        break\n",
    "ic(first_frame)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, frame in enumerate(video_info[video_order]['frames']):\n",
    "    if(first_frame['file_name'] == frame['file_name']):\n",
    "        prompt_frame_id = idx\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the prompt point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: sample more point\n",
    "\n",
    "def getSamplePointsFromMask(mask: np.ndarray) -> list:\n",
    "    kernel = np.ones((3, 3), np.uint8)  # 可以调整核的大小来控制闭运算程度\n",
    "\n",
    "# 对 mask 进行闭运算\n",
    "    closed_mask = cv2.morphologyEx(mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
    "        mask.astype(np.uint8)\n",
    "    )\n",
    "    center_points = []\n",
    "    # 遍历每个连通区域\n",
    "    for i in range(1, num_labels):  # 从 1 开始，因为 0 表示背景\n",
    "        # 获取中心坐标\n",
    "        sample_points = []\n",
    "        \n",
    "        center_x = centroids[i, 0]\n",
    "        center_y = centroids[i, 1]\n",
    "\n",
    "        sample_points.append([center_x, center_y])\n",
    "        # 将中心坐标添加到列表中\n",
    "        center_points.append(sample_points)\n",
    "        \n",
    "    return center_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_ids = coco.getAnnIds(imgIds=first_frame[\"id\"])\n",
    "anns = coco.loadAnns(ann_ids)\n",
    "ann_count = 0\n",
    "all_points = []\n",
    "all_masks = np.zeros((first_frame['height'],first_frame['width']))\n",
    "\n",
    "for ann in anns:\n",
    "    mask = coco.annToMask(ann)\n",
    "    all_masks[mask==1] = ann[\"category_id\"]\n",
    "    sample_points = getSamplePointsFromMask(mask)\n",
    "    \n",
    "    for reigon_samples in sample_points:\n",
    "        \n",
    "        labels = np.ones(len(reigon_samples))\n",
    "        points = np.array(reigon_samples)\n",
    "        # show_points(points, labels, plt.gca())\n",
    "        ann_obj_id = ann_count * (num_categories + 1) + ann[\"category_id\"]\n",
    "        # ic(ann_obj_id)\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=prompt_frame_id,\n",
    "            obj_id=ann_obj_id,\n",
    "            points=points,\n",
    "            labels=labels,\n",
    "        )\n",
    "        ann_count += 1\n",
    "        all_points.append(points)\n",
    "    # break    \n",
    "    \n",
    "# ic(anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('tab10')  # 'tab10' 提供 10 种不同的颜色\n",
    "\n",
    "for i, points in enumerate(all_points):\n",
    "    x = points[:, 0]  # 提取所有点的 x 坐标\n",
    "    y = points[:, 1]  # 提取所有点的 y 坐标\n",
    "    plt.scatter(x, y, color=cmap(i % 10))  # 绘制点，使用颜色映射自动分配颜色\n",
    "plt.imshow(all_masks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first prompt frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f\"frame {prompt_frame_id}\")\n",
    "plt.imshow(Image.open(video_info[video_order]['frames'][prompt_frame_id]['path']))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[2] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the whole video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_segments = {} \n",
    "# video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state, reverse=True):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_segments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frame_stride = 15\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_info[video_order]['frames']), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(video_info[video_order]['frames'][out_frame_idx]['path']))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_segments[29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use remainder to calculate the category and mask and save as COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_bbox(mask):\n",
    "    \"\"\"\n",
    "    Extracts the bounding box from a binary mask.\n",
    "    \"\"\"\n",
    "    pos = np.where(mask)\n",
    "    if len(pos[0]) == 0:\n",
    "        return None\n",
    "    xmin, ymin = np.min(pos[1]), np.min(pos[0])\n",
    "    xmax, ymax = np.max(pos[1]), np.max(pos[0])\n",
    "    return [float(xmin), float(ymin), float(xmax - xmin + 1), float(ymax - ymin + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_id in range(len(video_info[video_order][\"frames\"])):\n",
    "    current_frame = video_info[video_order][\"frames\"]\n",
    "    if current_frame[frame_id]['id'] == None:\n",
    "        continue\n",
    "\n",
    "    merged_mask = {}\n",
    "\n",
    "    ## merge the mask\n",
    "    for key, mask in video_segments[frame_id].items():\n",
    "        remainder = key % (num_categories + 1)\n",
    "        mask = np.logical_or.reduce(mask, axis=0)\n",
    "        if remainder not in merged_mask:\n",
    "            merged_mask[remainder] = mask\n",
    "        else:\n",
    "            merged_mask[remainder] = np.logical_or(merged_mask[remainder], mask)\n",
    "    # ic(merged_mask)\n",
    "    # break\n",
    "    for key, mask in merged_mask.items():\n",
    "        annotation = {\n",
    "            \"id\": uuid.uuid4(),\n",
    "            \"image_id\": current_frame[frame_id]['id'],\n",
    "            \"category_id\": key,\n",
    "            \"segmentation\": maskUtils.encode(np.asfortranarray(mask)),\n",
    "            \"bbox\": mask_to_bbox(mask),\n",
    "            \"area\": int(np.sum(mask)),\n",
    "            \"iscrowd\": 0,\n",
    "        }\n",
    "        coco_annotations.append(annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the result as COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ids = coco.getImgIds()\n",
    "cat_ids = coco.getCatIds()\n",
    "coco_images = coco.loadImgs(img_ids)\n",
    "coco_cats = coco.loadCats(cat_ids)\n",
    "predict_data = {\n",
    "    \"images\": coco_images,\n",
    "    \"annotations\": coco_annotations,\n",
    "    \"categories\": coco_cats\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
